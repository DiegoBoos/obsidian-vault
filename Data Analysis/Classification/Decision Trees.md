For two classes
Misclassification: $1-max(p,1-p)$ 
Gini: $2p(1-p)$
Cross-Entropy: $-plogp-(1-p)log(1-p)$

50,50
p= 50/100 = 0.5
1-p= 0.5

Gini: 2(0.5)(1-0.5)
= 2(0.5)(0.5)
= 2(0.25)
= 0.5

33,67

p=33/100= 0.33
1-p = 0.67
Misclassification: 1-(0.67)
Gini: 2(0.33)(0.67)
= 2(0.33)(0.67)
= 2(0.22)
= 0.44

Convergence
AIC: measure of error, lower is better
RD: measure of error, lower is better
ND: amount of randomly selected error
p-val (z-test)
Coefficients: need to be consistent with the correlation
